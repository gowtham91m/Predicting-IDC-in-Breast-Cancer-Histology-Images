{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowtham91m/Predicting-IDC-in-Breast-Cancer-Histology-Images/blob/master/IDC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dS9yxvA3_lxf",
        "colab_type": "code",
        "outputId": "fc26f395-f1a5-42f8-cd7f-60752e0a1bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Install Kaggle API for download competition data\n",
        "!pip3 install -q kaggle\n",
        "import os,shutil,fnmatch, random\n",
        "from glob import glob\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import keras\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, model_from_json, Model\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MVvQH83G4yFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mj86waWyIqNq",
        "colab_type": "code",
        "outputId": "6b22759a-a8a0-44a2-f33b-df60445bf892",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "root_dir = '/content'\n",
        "data_dir = os.path.join(root_dir,'IDC')\n",
        "download_dir = os.path.join(data_dir,'breast-histopathology-images')\n",
        "train_path = os.path.join(data_dir,'train')\n",
        "val_path = os.path.join(data_dir,'val')\n",
        "train_neg_path = os.path.join(train_path,'0')\n",
        "train_pos_path = os.path.join(train_path,'1')\n",
        "val_neg_path = os.path.join(val_path,'0')\n",
        "val_pos_path = os.path.join(val_path,'1')\n",
        "\n",
        "neg_class_images = os.path.join(download_dir, 'IDC_regular_ps50_idx5/**/0/*.png')\n",
        "pos_class_images = os.path.join(download_dir, 'IDC_regular_ps50_idx5/**/1/*.png')\n",
        "\n",
        "os.chdir(root_dir)\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = root_dir\n",
        "os.chdir(root_dir)\n",
        "if 'kaggle.json' not in os.listdir(root_dir):downloaded = files.upload()\n",
        "\n",
        "if 'IDC' not in os.listdir(root_dir):\n",
        "  os.mkdir(data_dir)\n",
        "  os.chdir(data_dir)\n",
        "\n",
        "  !kaggle datasets download -d paultimothymooney/breast-histopathology-images\n",
        "  !unzip -q -o breast-histopathology-images.zip -d breast-histopathology-images\n",
        "  os.chdir(download_dir)\n",
        "  !unzip -q -o IDC_regular_ps50_idx5.zip -d IDC_regular_ps50_idx5\n",
        "\n",
        "  if 'train' not in os.listdir(data_dir): os.mkdir(train_path)\n",
        "  if '0' not in os.listdir(train_path): os.mkdir(train_neg_path)\n",
        "  if '1' not in os.listdir(train_path): os.mkdir(train_pos_path)\n",
        "\n",
        "  if 'val' not in os.listdir(data_dir): os.mkdir(val_path)\n",
        "  if '0' not in os.listdir(val_path): os.mkdir(val_neg_path)\n",
        "  if '1' not in os.listdir(val_path): os.mkdir(val_pos_path)\n",
        "    \n",
        "  negative_class_images = glob(neg_class_images, recursive=True)  \n",
        "  positive_class_images = glob(pos_class_images, recursive=True)\n",
        "\n",
        "  for file in negative_class_images: shutil.copy2(file,train_neg_path)\n",
        "  for file in positive_class_images: shutil.copy2(file,train_pos_path)\n",
        "\n",
        "  # split train date into train and validation\n",
        "  train_neg_len = len(os.listdir(train_neg_path))\n",
        "  val_neg_len = train_neg_len * 0.3\n",
        "\n",
        "  train_pos_len = len(os.listdir(train_pos_path))\n",
        "  val_pos_len = train_pos_len * 0.3\n",
        "\n",
        "  val_pos = random.sample(os.listdir(train_pos_path),int(val_pos_len))\n",
        "  val_neg = random.sample(os.listdir(train_neg_path),int(val_neg_len))\n",
        "\n",
        "\n",
        "  for file in val_pos:\n",
        "    try: shutil.move(os.path.join(train_pos_path,file), val_pos_path)\n",
        "    except: pass\n",
        "  for file in val_neg:\n",
        "    try: shutil.move(os.path.join(train_neg_path,file), val_neg_path)\n",
        "    except: pass\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6df13e1d-1947-40de-b603-a99088b426fc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6df13e1d-1947-40de-b603-a99088b426fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /content/kaggle.json'\n",
            "Downloading breast-histopathology-images.zip to /content/IDC\n",
            "100% 1.48G/1.49G [00:14<00:00, 106MB/s]\n",
            "100% 1.49G/1.49G [00:14<00:00, 110MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QqZzJDph7sLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "ac4e980b-8a65-4e83-9633-aa92ff780a4f"
      },
      "cell_type": "code",
      "source": [
        "print('length of negative images',len(negative_class_images))\n",
        "print('length of positive images',len(positive_class_images))\n",
        "\n",
        "# y_train = [0]*len(negative_class_images) +[1]*len(positive_class_images)\n",
        "\n",
        "# class_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                  np.unique(y_train),\n",
        "#                                                  y_train)\n",
        "\n",
        "print('train negative images',len(os.listdir(train_neg_path)))\n",
        "print('train positive image',len(os.listdir(train_pos_path)))\n",
        "\n",
        "print('val negative images',len(os.listdir(val_neg_path)))\n",
        "print('val positive images',len(os.listdir(val_pos_path)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of negative images 198738\n",
            "length of positive images 78786\n",
            "train negative images 139117\n",
            "train positive image 55151\n",
            "val negative images 59621\n",
            "val positive images 23635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5AJyUIqYjuCN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ]
    },
    {
      "metadata": {
        "id": "XjXQqil8XcmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1400
        },
        "outputId": "148a091d-8356-4e5f-95b4-b339b91c8be9"
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=128\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,  # this is the target directory\n",
        "        target_size=(50, 50),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        target_size=(50, 50),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3, 3), input_shape=( 50, 50, 3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(BatchNormalization(axis=-1))\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('./base.model', monitor='val_loss', save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
        "reduceLROnPlato = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='min')\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator, \n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "         callbacks=[checkpoint, reduceLROnPlato, EarlyStopping(patience=8)])\n",
        "model.save_weights('cnn.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 194268 images belonging to 2 classes.\n",
            "Found 83256 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "1518/1518 [==============================] - 366s 241ms/step - loss: 0.4325 - acc: 0.8184 - val_loss: 0.3650 - val_acc: 0.8386\n",
            "Epoch 2/50\n",
            "1518/1518 [==============================] - 354s 233ms/step - loss: 0.3790 - acc: 0.8431 - val_loss: 0.3380 - val_acc: 0.8545\n",
            "Epoch 3/50\n",
            "1518/1518 [==============================] - 354s 233ms/step - loss: 0.3627 - acc: 0.8510 - val_loss: 0.3287 - val_acc: 0.8587\n",
            "Epoch 4/50\n",
            "1518/1518 [==============================] - 354s 233ms/step - loss: 0.3499 - acc: 0.8561 - val_loss: 0.3492 - val_acc: 0.8490\n",
            "Epoch 5/50\n",
            "1518/1518 [==============================] - 355s 234ms/step - loss: 0.3370 - acc: 0.8621 - val_loss: 0.3114 - val_acc: 0.8695\n",
            "Epoch 6/50\n",
            "1518/1518 [==============================] - 354s 234ms/step - loss: 0.3309 - acc: 0.8627 - val_loss: 0.3030 - val_acc: 0.8712\n",
            "Epoch 7/50\n",
            "1518/1518 [==============================] - 355s 234ms/step - loss: 0.3247 - acc: 0.8652 - val_loss: 0.3034 - val_acc: 0.8702\n",
            "Epoch 8/50\n",
            "1518/1518 [==============================] - 355s 234ms/step - loss: 0.3233 - acc: 0.8666 - val_loss: 0.3074 - val_acc: 0.8709\n",
            "Epoch 9/50\n",
            "1518/1518 [==============================] - 355s 234ms/step - loss: 0.3169 - acc: 0.8699 - val_loss: 0.2934 - val_acc: 0.8749\n",
            "Epoch 10/50\n",
            "1518/1518 [==============================] - 355s 234ms/step - loss: 0.3134 - acc: 0.8734 - val_loss: 0.2979 - val_acc: 0.8762\n",
            "Epoch 11/50\n",
            "1518/1518 [==============================] - 354s 233ms/step - loss: 0.3093 - acc: 0.8751 - val_loss: 0.2904 - val_acc: 0.8769\n",
            "Epoch 12/50\n",
            "1518/1518 [==============================] - 351s 231ms/step - loss: 0.3057 - acc: 0.8766 - val_loss: 0.2871 - val_acc: 0.8778\n",
            "Epoch 13/50\n",
            "1518/1518 [==============================] - 361s 238ms/step - loss: 0.3019 - acc: 0.8782 - val_loss: 0.2838 - val_acc: 0.8791\n",
            "Epoch 14/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.3007 - acc: 0.8780 - val_loss: 0.2816 - val_acc: 0.8803\n",
            "Epoch 15/50\n",
            "1518/1518 [==============================] - 357s 235ms/step - loss: 0.2977 - acc: 0.8800 - val_loss: 0.2907 - val_acc: 0.8792\n",
            "Epoch 16/50\n",
            "1518/1518 [==============================] - 357s 235ms/step - loss: 0.2958 - acc: 0.8806 - val_loss: 0.2835 - val_acc: 0.8801\n",
            "Epoch 17/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2932 - acc: 0.8826 - val_loss: 0.2722 - val_acc: 0.8835\n",
            "Epoch 18/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2934 - acc: 0.8819 - val_loss: 0.2798 - val_acc: 0.8821\n",
            "Epoch 19/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2913 - acc: 0.8826 - val_loss: 0.2933 - val_acc: 0.8769\n",
            "Epoch 20/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2894 - acc: 0.8831 - val_loss: 0.2843 - val_acc: 0.8814\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 21/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2737 - acc: 0.8894 - val_loss: 0.2685 - val_acc: 0.8860\n",
            "Epoch 22/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2710 - acc: 0.8907 - val_loss: 0.2690 - val_acc: 0.8864\n",
            "Epoch 23/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2698 - acc: 0.8899 - val_loss: 0.2638 - val_acc: 0.8878\n",
            "Epoch 24/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2684 - acc: 0.8916 - val_loss: 0.2643 - val_acc: 0.8889\n",
            "Epoch 25/50\n",
            "1518/1518 [==============================] - 358s 236ms/step - loss: 0.2681 - acc: 0.8913 - val_loss: 0.2637 - val_acc: 0.8883\n",
            "Epoch 26/50\n",
            "1518/1518 [==============================] - 356s 234ms/step - loss: 0.2678 - acc: 0.8919 - val_loss: 0.2626 - val_acc: 0.8888\n",
            "Epoch 27/50\n",
            "1518/1518 [==============================] - 357s 235ms/step - loss: 0.2666 - acc: 0.8924 - val_loss: 0.2647 - val_acc: 0.8884\n",
            "Epoch 28/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2662 - acc: 0.8924 - val_loss: 0.2631 - val_acc: 0.8885\n",
            "Epoch 29/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2652 - acc: 0.8924 - val_loss: 0.2614 - val_acc: 0.8886\n",
            "Epoch 30/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2659 - acc: 0.8920 - val_loss: 0.2617 - val_acc: 0.8893\n",
            "Epoch 31/50\n",
            "1518/1518 [==============================] - 356s 235ms/step - loss: 0.2655 - acc: 0.8924 - val_loss: 0.2616 - val_acc: 0.8893\n",
            "Epoch 32/50\n",
            "1518/1518 [==============================] - 357s 235ms/step - loss: 0.2640 - acc: 0.8922 - val_loss: 0.2600 - val_acc: 0.8897\n",
            "Epoch 33/50\n",
            "1518/1518 [==============================] - 354s 233ms/step - loss: 0.2636 - acc: 0.8932 - val_loss: 0.2599 - val_acc: 0.8906\n",
            "Epoch 34/50\n",
            "1518/1518 [==============================] - 352s 232ms/step - loss: 0.2631 - acc: 0.8935 - val_loss: 0.2597 - val_acc: 0.8895\n",
            "Epoch 35/50\n",
            "1518/1518 [==============================] - 352s 232ms/step - loss: 0.2631 - acc: 0.8931 - val_loss: 0.2615 - val_acc: 0.8886\n",
            "Epoch 36/50\n",
            "1518/1518 [==============================] - 352s 232ms/step - loss: 0.2636 - acc: 0.8931 - val_loss: 0.2582 - val_acc: 0.8899\n",
            "Epoch 37/50\n",
            "1518/1518 [==============================] - 352s 232ms/step - loss: 0.2616 - acc: 0.8938 - val_loss: 0.2572 - val_acc: 0.8897\n",
            "Epoch 38/50\n",
            "1518/1518 [==============================] - 352s 232ms/step - loss: 0.2626 - acc: 0.8936 - val_loss: 0.2599 - val_acc: 0.8898\n",
            "Epoch 39/50\n",
            " 624/1518 [===========>..................] - ETA: 3:01 - loss: 0.2615 - acc: 0.8943Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrwQn3bbj2ax",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "metadata": {
        "id": "oGdnGrZgpfdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BASE_MODEL = 'VGG16'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def transfer_learning(BASE_MODEL,optimizer):\n",
        "  if BASE_MODEL=='VGG16':\n",
        "      from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='vgg19':\n",
        "      from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='ResNet50':\n",
        "      from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='InceptionV3':\n",
        "      from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='Xception':\n",
        "      from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet169': \n",
        "      from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet121':\n",
        "      from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "  else:\n",
        "      raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "  \n",
        "  import keras\n",
        "  keras.backend.set_learning_phase(1)\n",
        "  \n",
        "  check_point_name = BASE_MODEL + '.model'\n",
        "  model_weights = BASE_MODEL + '.h5'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          shear_range=0.2,\n",
        "          zoom_range=0.2,\n",
        "          horizontal_flip=True,\n",
        "          preprocessing_function = preprocess_input)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  preprocessing_function = preprocess_input)\n",
        "\n",
        "  batch_size=128\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          train_path,  # this is the target directory\n",
        "          target_size=(250, 250),  # all images will be resized to 150x150\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary') \n",
        "\n",
        "  validation_generator = val_datagen.flow_from_directory(\n",
        "          val_path,\n",
        "          target_size=(250, 250),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary')    \n",
        "\n",
        "  img_rows, img_cols, img_channel = 250, 250, 3\n",
        "  base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "  add_model = Sequential()\n",
        "  add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "  add_model.add(Dense(64, activation='relu'))\n",
        "  add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "      if layer.name.startswith('bn'):\n",
        "          layer.call(layer.input, training=False)\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "  start_time = time()\n",
        "  model.fit_generator(\n",
        "          train_generator,\n",
        "          epochs=8,\n",
        "          validation_data=validation_generator,\n",
        "          #class_weight = class_weights,\n",
        "          callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "  model.save_weights(model_weights)\n",
        "\n",
        "  print('time taken ',time()-start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GN_fmKT7Dr__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "018ee98b-7203-4e91-df80-b41d637f04b7"
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "transfer_learning('VGG16', optimizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 194268 images belonging to 2 classes.\n",
            "Found 83256 images belonging to 2 classes.\n",
            "Epoch 1/8\n",
            "6071/6071 [==============================] - 4837s 797ms/step - loss: 0.3744 - acc: 0.8356 - val_loss: 0.3752 - val_acc: 0.8323\n",
            "Epoch 2/8\n",
            "6071/6071 [==============================] - 4864s 801ms/step - loss: 0.3541 - acc: 0.8452 - val_loss: 0.3496 - val_acc: 0.8463\n",
            "Epoch 3/8\n",
            "6071/6071 [==============================] - 4787s 788ms/step - loss: 0.3454 - acc: 0.8496 - val_loss: 0.3467 - val_acc: 0.8485\n",
            "Epoch 4/8\n",
            "6071/6071 [==============================] - 4790s 789ms/step - loss: 0.3393 - acc: 0.8522 - val_loss: 0.3391 - val_acc: 0.8512\n",
            "Epoch 5/8\n",
            "3378/6071 [===============>..............] - ETA: 28:23 - loss: 0.3353 - acc: 0.8547"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TT72PNuI4H_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "8de90a3d-7f1f-4c8a-eac1-58bc1d817753"
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "transfer_learning('DenseNet169', optimizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 194268 images belonging to 2 classes.\n",
            "Found 83256 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "53182464/53178568 [==============================] - 5s 0us/step\n",
            "Epoch 1/8\n",
            "1518/1518 [==============================] - 4472s 3s/step - loss: 0.3247 - acc: 0.8610 - val_loss: 0.3129 - val_acc: 0.8656\n",
            "Epoch 2/8\n",
            "1518/1518 [==============================] - 4523s 3s/step - loss: 0.3045 - acc: 0.8701 - val_loss: 0.3250 - val_acc: 0.8611\n",
            "Epoch 3/8\n",
            "1518/1518 [==============================] - 4443s 3s/step - loss: 0.2996 - acc: 0.8723 - val_loss: 0.3018 - val_acc: 0.8705\n",
            "Epoch 4/8\n",
            "1517/1518 [============================>.] - ETA: 2s - loss: 0.2959 - acc: 0.8734"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "84ALLsArwDZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf   \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "\n",
        "def transfer_learning(BASE_MODEL,optimizer):\n",
        "  if BASE_MODEL=='VGG16':\n",
        "      from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='vgg19':\n",
        "      from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='ResNet50':\n",
        "      from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='InceptionV3':\n",
        "      from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='Xception':\n",
        "      from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet169': \n",
        "      from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet121':\n",
        "      from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "  else:\n",
        "      raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "  \n",
        "  import keras\n",
        "  keras.backend.set_learning_phase(1)\n",
        "  \n",
        "  check_point_name = BASE_MODEL + '.model'\n",
        "  model_weights = BASE_MODEL + '.h5'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          shear_range=0.2,\n",
        "          zoom_range=0.2,\n",
        "          horizontal_flip=True,\n",
        "          preprocessing_function = preprocess_input)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  preprocessing_function = preprocess_input)\n",
        "\n",
        "  batch_size=128\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          train_path,  # this is the target directory\n",
        "          target_size=(250, 250),  # all images will be resized to 150x150\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary') \n",
        "\n",
        "  validation_generator = val_datagen.flow_from_directory(\n",
        "          val_path,\n",
        "          target_size=(250, 250),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary')    \n",
        "\n",
        "  img_rows, img_cols, img_channel = 250, 250, 3\n",
        "  base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "  add_model = Sequential()\n",
        "  add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "  add_model.add(Dense(64, activation='relu'))\n",
        "  add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "      if layer.name.startswith('bn'):\n",
        "          layer.call(layer.input, training=False)\n",
        "\n",
        "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                                                model,\n",
        "                                                strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                                                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "  tpu_model.compile(\n",
        "      optimizer=tf.train.AdamOptimizer(learning_rate = 0.001),   \n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy'])          \n",
        "\n",
        "  \n",
        "  start_time = time()\n",
        "  tpu_model.fit_generator(\n",
        "          train_generator,\n",
        "          epochs=8,\n",
        "          validation_data=validation_generator,\n",
        "          #class_weight = class_weights,\n",
        "          callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "  model.save_weights(model_weights)\n",
        "\n",
        "  print('time taken ',time()-start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHxbkqfn-AJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "outputId": "7eab6d72-c746-4bbd-802e-abec3bfb0dc5"
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "transfer_learning('DenseNet169', optimizer)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 194268 images belonging to 2 classes.\n",
            "Found 83256 images belonging to 2 classes.\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.92.91.162:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11139267846929878533)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4851780415960072533)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1961844471428908267)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11077741763262080519)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14524357382309376833)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7318996246941685864)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7727673847298168158)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 17635836928529441130)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16857560464086933465)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17480430179907998627)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 13660684951841252086)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 18259598018525437552)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7955ef670f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransfer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DenseNet169'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-e675941787aa>\u001b[0m in \u001b[0;36mtransfer_learning\u001b[0;34m(BASE_MODEL, optimizer)\u001b[0m\n\u001b[1;32m     73\u001b[0m                                                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                                                 strategy=tf.contrib.tpu.TPUDistributionStrategy(\n\u001b[0;32m---> 75\u001b[0;31m                                                 tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   tpu_model.compile(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/framework/experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m'any time, and without warning.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         decorator_utils.get_qualified_name(func), func.__module__)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   new_func.__doc__ = _add_experimental_function_notice_to_docstring(\n\u001b[1;32m     66\u001b[0m       func.__doc__)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mtpu_model\u001b[0;34m(model, strategy)\u001b[0m\n\u001b[1;32m   2143\u001b[0m   \u001b[0;31m# If the model has already been initialized, grab the optimizer configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m   \u001b[0;31m# and model weights before entering the TPU session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2146\u001b[0m     if (isinstance(model.optimizer, keras_optimizers.Optimizer) and not\n\u001b[1;32m   2147\u001b[0m         isinstance(model.optimizer, keras_optimizers.TFOptimizer)):\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'optimizer'"
          ]
        }
      ]
    }
  ]
}